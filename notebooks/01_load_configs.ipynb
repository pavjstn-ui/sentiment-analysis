{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070bd631-e011-45e3-a38b-66b559615e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 test cases\n",
      "Model: xlm-roberta-base\n",
      "Provider: huggingface\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load test cases (all documents)\n",
    "with open('suites/basic/sentiment_tests.yaml') as f:\n",
    "    tests = list(yaml.safe_load_all(f))\n",
    "\n",
    "# Load config\n",
    "with open('configs/xlm_roberta_baseline.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Loaded {len(tests)} test cases\")\n",
    "print(f\"Model: {config['model']}\")\n",
    "print(f\"Provider: {config['provider']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0dc0297-3139-4a5f-a7a7-235433e1193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== First Test Case ===\n",
      "{'id': 'sentiment_001', 'task': 'Classify the sentiment of this Czech review as POSITIVE, NEGATIVE, or NEUTRAL.', 'input': 'Výborný produkt! Velmi spokojený s kvalitou a dodáním.', 'expected': 'POSITIVE', 'rubric': {'accuracy': {'weight': 0.8, 'rule': 'exact_match'}, 'confidence': {'weight': 0.2, 'rule': 'score_above_0.85'}}, 'tags': ['domain:sentiment', 'language:czech', 'difficulty:easy', 'risk_level:low']}\n",
      "\n",
      "\n",
      "=== Config Details ===\n",
      "name: xlm-roberta-baseline\n",
      "description: XLM-RoBERTa base model - no fine-tuning\n",
      "provider: huggingface\n",
      "model: xlm-roberta-base\n",
      "task_type: text-classification\n",
      "num_labels: 3\n",
      "temperature: 0.3\n",
      "max_tokens: 50\n",
      "batch_size: 8\n",
      "seed: 42\n",
      "fine_tuned_weights: None\n"
     ]
    }
   ],
   "source": [
    "# Explore test cases\n",
    "print(\"=== First Test Case ===\")\n",
    "print(tests[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore config\n",
    "print(\"=== Config Details ===\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0808ac0-94c8-4795-b5c4-6a52098238cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: sentiment_001 - POSITIVE\n",
      "Test 2: sentiment_002 - NEGATIVE\n",
      "Test 3: sentiment_003 - NEUTRAL\n",
      "Test 4: sentiment_004 - NEGATIVE\n",
      "Test 5: sentiment_005 - POSITIVE\n",
      "Test 6: sentiment_006 - POSITIVE\n",
      "Test 7: sentiment_007 - NEGATIVE\n",
      "Test 8: sentiment_008 - NEGATIVE\n",
      "Test 9: sentiment_009 - POSITIVE\n",
      "Test 10: sentiment_010 - NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# See all test cases\n",
    "for i, test in enumerate(tests):\n",
    "    print(f\"Test {i+1}: {test['id']} - {test['expected']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9aad6c-ef1c-452f-84ea-bc6ffbc21def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationRunner:\n",
    "    \"\"\"Runs tests against a model config and scores results\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.results = []\n",
    "    \n",
    "    def run_test(self, test, mock_output=None):\n",
    "        \"\"\"\n",
    "        Run a single test case\n",
    "        mock_output: For now, we mock the model output\n",
    "        \"\"\"\n",
    "        if mock_output is None:\n",
    "            # Placeholder: later this will call the actual model\n",
    "            mock_output = test['expected']  # Mock: always returns expected\n",
    "        \n",
    "        # Score the test\n",
    "        score = self.score_test(test, mock_output)\n",
    "        \n",
    "        result = {\n",
    "            'id': test['id'],\n",
    "            'input': test['input'],\n",
    "            'expected': test['expected'],\n",
    "            'actual': mock_output,\n",
    "            'score': score,\n",
    "            'passed': score >= 0.8\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def score_test(self, test, actual_output):\n",
    "        \"\"\"Score based on rubric\"\"\"\n",
    "        rubric = test['rubric']\n",
    "        total_score = 0\n",
    "        \n",
    "        for criterion_name, criterion in rubric.items():\n",
    "            weight = criterion['weight']\n",
    "            rule = criterion['rule']\n",
    "            \n",
    "            # For now: exact_match rule\n",
    "            if rule == 'exact_match':\n",
    "                criterion_score = 1.0 if actual_output == test['expected'] else 0.0\n",
    "            else:\n",
    "                criterion_score = 1.0  # Placeholder for other rules\n",
    "            \n",
    "            total_score += criterion_score * weight\n",
    "        \n",
    "        return total_score\n",
    "    \n",
    "    def run_suite(self, tests, mock_outputs=None):\n",
    "        \"\"\"Run all tests in a suite\"\"\"\n",
    "        for i, test in enumerate(tests):\n",
    "            mock_output = mock_outputs[i] if mock_outputs else None\n",
    "            self.run_test(test, mock_output)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print summary of results\"\"\"\n",
    "        total = len(self.results)\n",
    "        passed = sum(1 for r in self.results if r['passed'])\n",
    "        avg_score = sum(r['score'] for r in self.results) / total if total > 0 else 0\n",
    "        \n",
    "        print(f\"\\n=== Evaluation Summary ===\")\n",
    "        print(f\"Model: {self.config['model']}\")\n",
    "        print(f\"Tests run: {total}\")\n",
    "        print(f\"Passed: {passed}/{total}\")\n",
    "        print(f\"Average score: {avg_score:.2%}\")\n",
    "        print()\n",
    "        \n",
    "        for result in self.results:\n",
    "            status = \"✓\" if result['passed'] else \"✗\"\n",
    "            print(f\"{status} {result['id']}: {result['actual']} (expected: {result['expected']}, score: {result['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5ecaac-9cd1-48f6-8e80-b1fc80cd0539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Summary ===\n",
      "Model: xlm-roberta-base\n",
      "Tests run: 10\n",
      "Passed: 10/10\n",
      "Average score: 100.00%\n",
      "\n",
      "✓ sentiment_001: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_002: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_003: NEUTRAL (expected: NEUTRAL, score: 1.00)\n",
      "✓ sentiment_004: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_005: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_006: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_007: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_008: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_009: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_010: NEGATIVE (expected: NEGATIVE, score: 1.00)\n"
     ]
    }
   ],
   "source": [
    "# Create runner\n",
    "runner = EvaluationRunner(config)\n",
    "\n",
    "# Run all tests (mocked for now)\n",
    "results = runner.run_suite(tests)\n",
    "\n",
    "# Print summary\n",
    "runner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79831ea-c5b6-41ab-b045-153aed28163a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
