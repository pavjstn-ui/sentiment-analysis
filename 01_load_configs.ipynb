{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82eca450-22e5-47a7-a172-11e9e4962f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36abef89-f439-4f8c-ba4e-18b5cbd37aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 test cases\n",
      "Model: xlm-roberta-base\n",
      "Provider: huggingface\n"
     ]
    }
   ],
   "source": [
    "# Load test cases\n",
    "with open('suites/basic/sentiment_tests.yaml') as f:\n",
    "    tests = list(yaml.safe_load_all(f))\n",
    "\n",
    "# Load config\n",
    "with open('configs/xlm_roberta_baseline.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Loaded {len(tests)} test cases\")\n",
    "print(f\"Model: {config['model']}\")\n",
    "print(f\"Provider: {config['provider']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070bd631-e011-45e3-a38b-66b559615e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 test cases\n",
      "Model: xlm-roberta-base\n",
      "Provider: huggingface\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load test cases (all documents)\n",
    "with open('suites/basic/sentiment_tests.yaml') as f:\n",
    "    tests = list(yaml.safe_load_all(f))\n",
    "\n",
    "# Load config\n",
    "with open('configs/xlm_roberta_baseline.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Loaded {len(tests)} test cases\")\n",
    "print(f\"Model: {config['model']}\")\n",
    "print(f\"Provider: {config['provider']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0dc0297-3139-4a5f-a7a7-235433e1193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== First Test Case ===\n",
      "{'id': 'sentiment_001', 'task': 'Classify the sentiment of this Czech review as POSITIVE, NEGATIVE, or NEUTRAL.', 'input': 'Výborný produkt! Velmi spokojený s kvalitou a dodáním.', 'expected': 'POSITIVE', 'rubric': {'accuracy': {'weight': 0.8, 'rule': 'exact_match'}, 'confidence': {'weight': 0.2, 'rule': 'score_above_0.85'}}, 'tags': ['domain:sentiment', 'language:czech', 'difficulty:easy', 'risk_level:low']}\n",
      "\n",
      "\n",
      "=== Config Details ===\n",
      "name: xlm-roberta-baseline\n",
      "description: XLM-RoBERTa base model - no fine-tuning\n",
      "provider: huggingface\n",
      "model: xlm-roberta-base\n",
      "task_type: text-classification\n",
      "num_labels: 3\n",
      "temperature: 0.3\n",
      "max_tokens: 50\n",
      "batch_size: 8\n",
      "seed: 42\n",
      "fine_tuned_weights: None\n"
     ]
    }
   ],
   "source": [
    "# Explore test cases\n",
    "print(\"=== First Test Case ===\")\n",
    "print(tests[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore config\n",
    "print(\"=== Config Details ===\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0808ac0-94c8-4795-b5c4-6a52098238cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: sentiment_001 - POSITIVE\n",
      "Test 2: sentiment_002 - NEGATIVE\n",
      "Test 3: sentiment_003 - NEUTRAL\n",
      "Test 4: sentiment_004 - NEGATIVE\n",
      "Test 5: sentiment_005 - POSITIVE\n",
      "Test 6: sentiment_006 - POSITIVE\n",
      "Test 7: sentiment_007 - NEGATIVE\n",
      "Test 8: sentiment_008 - NEGATIVE\n",
      "Test 9: sentiment_009 - POSITIVE\n",
      "Test 10: sentiment_010 - NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# See all test cases\n",
    "for i, test in enumerate(tests):\n",
    "    print(f\"Test {i+1}: {test['id']} - {test['expected']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9aad6c-ef1c-452f-84ea-bc6ffbc21def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationRunner:\n",
    "    \"\"\"Runs tests against a model config and scores results\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.results = []\n",
    "    \n",
    "    def run_test(self, test, mock_output=None):\n",
    "        \"\"\"\n",
    "        Run a single test case\n",
    "        mock_output: For now, we mock the model output\n",
    "        \"\"\"\n",
    "        if mock_output is None:\n",
    "            # Placeholder: later this will call the actual model\n",
    "            mock_output = test['expected']  # Mock: always returns expected\n",
    "        \n",
    "        # Score the test\n",
    "        score = self.score_test(test, mock_output)\n",
    "        \n",
    "        result = {\n",
    "            'id': test['id'],\n",
    "            'input': test['input'],\n",
    "            'expected': test['expected'],\n",
    "            'actual': mock_output,\n",
    "            'score': score,\n",
    "            'passed': score >= 0.8\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def score_test(self, test, actual_output):\n",
    "        \"\"\"Score based on rubric\"\"\"\n",
    "        rubric = test['rubric']\n",
    "        total_score = 0\n",
    "        \n",
    "        for criterion_name, criterion in rubric.items():\n",
    "            weight = criterion['weight']\n",
    "            rule = criterion['rule']\n",
    "            \n",
    "            # For now: exact_match rule\n",
    "            if rule == 'exact_match':\n",
    "                criterion_score = 1.0 if actual_output == test['expected'] else 0.0\n",
    "            else:\n",
    "                criterion_score = 1.0  # Placeholder for other rules\n",
    "            \n",
    "            total_score += criterion_score * weight\n",
    "        \n",
    "        return total_score\n",
    "    \n",
    "    def run_suite(self, tests, mock_outputs=None):\n",
    "        \"\"\"Run all tests in a suite\"\"\"\n",
    "        for i, test in enumerate(tests):\n",
    "            mock_output = mock_outputs[i] if mock_outputs else None\n",
    "            self.run_test(test, mock_output)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print summary of results\"\"\"\n",
    "        total = len(self.results)\n",
    "        passed = sum(1 for r in self.results if r['passed'])\n",
    "        avg_score = sum(r['score'] for r in self.results) / total if total > 0 else 0\n",
    "        \n",
    "        print(f\"\\n=== Evaluation Summary ===\")\n",
    "        print(f\"Model: {self.config['model']}\")\n",
    "        print(f\"Tests run: {total}\")\n",
    "        print(f\"Passed: {passed}/{total}\")\n",
    "        print(f\"Average score: {avg_score:.2%}\")\n",
    "        print()\n",
    "        \n",
    "        for result in self.results:\n",
    "            status = \"✓\" if result['passed'] else \"✗\"\n",
    "            print(f\"{status} {result['id']}: {result['actual']} (expected: {result['expected']}, score: {result['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5ecaac-9cd1-48f6-8e80-b1fc80cd0539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Summary ===\n",
      "Model: xlm-roberta-base\n",
      "Tests run: 10\n",
      "Passed: 10/10\n",
      "Average score: 100.00%\n",
      "\n",
      "✓ sentiment_001: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_002: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_003: NEUTRAL (expected: NEUTRAL, score: 1.00)\n",
      "✓ sentiment_004: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_005: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_006: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_007: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_008: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_009: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_010: NEGATIVE (expected: NEGATIVE, score: 1.00)\n"
     ]
    }
   ],
   "source": [
    "# Create runner\n",
    "runner = EvaluationRunner(config)\n",
    "\n",
    "# Run all tests (mocked for now)\n",
    "results = runner.run_suite(tests)\n",
    "\n",
    "# Print summary\n",
    "runner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b20eebf-7ea2-4141-93c3-1cfa3640535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e74427-bc97-46eb-9712-372cd71b4ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/macski/claude/ai-evaluator-project/notebooks\n",
      "\n",
      "Files in suites/basic/:\n",
      "\n",
      "ls: suites/basic/: No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(\"\\nFiles in suites/basic/:\")\n",
    "import subprocess\n",
    "result = subprocess.run(['ls', '-la', 'suites/basic/'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8309677f-5627-4823-b0d5-cf7bf40a43fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 test cases\n"
     ]
    }
   ],
   "source": [
    "# Load test cases\n",
    "with open('suites/basic/sentiment_tests.yaml') as f:\n",
    "    tests = list(yaml.safe_load_all(f))\n",
    "\n",
    "# Load config\n",
    "with open('configs/xlm_roberta_baseline.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Loaded {len(tests)} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d399d203-29e9-4fcb-b710-6197a822a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationRunner:\n",
    "    \"\"\"Runs tests against a model config and scores results\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.results = []\n",
    "    \n",
    "    def run_test(self, test, mock_output=None):\n",
    "        if mock_output is None:\n",
    "            mock_output = test['expected']\n",
    "        \n",
    "        score = self.score_test(test, mock_output)\n",
    "        \n",
    "        result = {\n",
    "            'id': test['id'],\n",
    "            'input': test['input'],\n",
    "            'expected': test['expected'],\n",
    "            'actual': mock_output,\n",
    "            'score': score,\n",
    "            'passed': score >= 0.8\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def score_test(self, test, actual_output):\n",
    "        rubric = test['rubric']\n",
    "        total_score = 0\n",
    "        \n",
    "        for criterion_name, criterion in rubric.items():\n",
    "            weight = criterion['weight']\n",
    "            rule = criterion['rule']\n",
    "            \n",
    "            if rule == 'exact_match':\n",
    "                criterion_score = 1.0 if actual_output == test['expected'] else 0.0\n",
    "            else:\n",
    "                criterion_score = 1.0\n",
    "            \n",
    "            total_score += criterion_score * weight\n",
    "        \n",
    "        return total_score\n",
    "    \n",
    "    def run_suite(self, tests, mock_outputs=None):\n",
    "        for i, test in enumerate(tests):\n",
    "            mock_output = mock_outputs[i] if mock_outputs else None\n",
    "            self.run_test(test, mock_output)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def summary(self):\n",
    "        total = len(self.results)\n",
    "        passed = sum(1 for r in self.results if r['passed'])\n",
    "        avg_score = sum(r['score'] for r in self.results) / total if total > 0 else 0\n",
    "        \n",
    "        print(f\"\\n=== Evaluation Summary ===\")\n",
    "        print(f\"Model: {self.config['model']}\")\n",
    "        print(f\"Tests run: {total}\")\n",
    "        print(f\"Passed: {passed}/{total}\")\n",
    "        print(f\"Average score: {avg_score:.2%}\")\n",
    "        print()\n",
    "        \n",
    "        for result in self.results:\n",
    "            status = \"✓\" if result['passed'] else \"✗\"\n",
    "            print(f\"{status} {result['id']}: {result['actual']} (expected: {result['expected']}, score: {result['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2572f4e8-21c3-49f4-bf14-16747b8c6a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Summary ===\n",
      "Model: xlm-roberta-base\n",
      "Tests run: 10\n",
      "Passed: 10/10\n",
      "Average score: 100.00%\n",
      "\n",
      "✓ sentiment_001: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_002: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_003: NEUTRAL (expected: NEUTRAL, score: 1.00)\n",
      "✓ sentiment_004: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_005: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_006: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_007: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_008: NEGATIVE (expected: NEGATIVE, score: 1.00)\n",
      "✓ sentiment_009: POSITIVE (expected: POSITIVE, score: 1.00)\n",
      "✓ sentiment_010: NEGATIVE (expected: NEGATIVE, score: 1.00)\n"
     ]
    }
   ],
   "source": [
    "# Create runner\n",
    "runner = EvaluationRunner(config)\n",
    "\n",
    "# Run all tests\n",
    "results = runner.run_suite(tests)\n",
    "\n",
    "# Print summary\n",
    "runner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e79831ea-c5b6-41ab-b045-153aed28163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: runs/20251213_113443_xlm-roberta-baseline\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def save_results(runner, suite_name):\n",
    "    \"\"\"Save evaluation results to runs/ folder\"\"\"\n",
    "    \n",
    "    # Create run folder with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_id = f\"{timestamp}_{runner.config['name']}\"\n",
    "    run_folder = Path(f\"runs/{run_id}\")\n",
    "    run_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save results as JSON\n",
    "    results_json = {\n",
    "        'run_id': run_id,\n",
    "        'timestamp': timestamp,\n",
    "        'config': runner.config,\n",
    "        'suite': suite_name,\n",
    "        'summary': {\n",
    "            'total': len(runner.results),\n",
    "            'passed': sum(1 for r in runner.results if r['passed']),\n",
    "            'avg_score': sum(r['score'] for r in runner.results) / len(runner.results) if runner.results else 0\n",
    "        },\n",
    "        'results': runner.results\n",
    "    }\n",
    "    \n",
    "    with open(run_folder / 'results.json', 'w') as f:\n",
    "        json.dump(results_json, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Results saved to: {run_folder}\")\n",
    "    return run_folder\n",
    "\n",
    "# Test it\n",
    "run_folder = save_results(runner, 'basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e9ac4-9206-4d86-89b8-9c3348200a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf80b6-86c0-469a-bdc4-bebe77cb02ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
